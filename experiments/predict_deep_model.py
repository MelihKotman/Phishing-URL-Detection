import os
import sys
import tensorflow
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # TensorFlow uyarÄ±larÄ±nÄ± bastÄ±rÄ±r

import pickle
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences  # type: ignore
import numpy as np

# --- AYARLAR (EÄŸitimdekiyle AYNI) ---
MAX_LEN = 75

def load_ai_assets():
    """
    EÄŸitim sÄ±rasÄ±nda kaydedilen model ve tokenizer dosyalarÄ±nÄ± yÃ¼kler.
    """

    print("Model YÃ¼kleniyor...")

    # Modeli yÃ¼kle
    try: 
        model = (tf.keras.models.load_model('src_db/model/phishing_model.keras'))  # type: ignore
    except OSError as e:
        print("Model dosyasÄ± bulunamadÄ±. LÃ¼tfen modeli eÄŸitip kaydedin.")
        raise e
    
    # Tokenizer'Ä± yÃ¼kle
    try:
        with open('src_db/model/tokenizer.pickle', 'rb') as handle: 
            tokenizer = pickle.load(handle)
    except FileNotFoundError as e:
        print("Tokenizer dosyasÄ± bulunamadÄ±. LÃ¼tfen modeli eÄŸitip kaydedin.")
        raise e
    return model, tokenizer
    
def predict_url(url, model, tokenizer):
    """Verilen URL'in Phishing olup olmadÄ±ÄŸÄ±nÄ± tahmin eder."""
    WHITELIST_DOMAINS = {
    "google.com", "www.google.com", "youtube.com", "www.youtube.com",
    "facebook.com", "www.facebook.com", "amazon.com", "twitter.com",
    "instagram.com", "linkedin.com", "wikipedia.org", "yahoo.com",
    "yandex.com", "yandex.ru", "whatsapp.com", "bing.com", "live.com",
    "microsoft.com", "apple.com", "github.com", "stackoverflow.com",
    "ibu.edu.tr", "www.ibu.edu.tr", "turkiye.gov.tr", "enabiz.gov.tr"
}   
    clean_url = url.replace("https://", "").replace("http://", "").replace("www.", "").strip("/")
    
    # 1. Beyaz Liste KontrolÃ¼ (Yapay Zekadan Ã–nce)
    if clean_url in WHITELIST_DOMAINS:
        print(f"\nğŸ” Analiz Edilen: {url}")
        print("ğŸ›¡ï¸  SONUÃ‡: GÃœVENLÄ° (Beyaz Listede Mevcut)")
        print("   -> Yapay zeka yorulmadÄ±, bilinen gÃ¼venli site.")
        print("-" * 40)
        return

    # 1. Ã–n Ä°ÅŸleme (Preprocessing)
    # URL'i string yap, listeye koy (Tokenizer liste bekler)
    sequences = tokenizer.texts_to_sequences([str(clean_url)])
    
    # UzunluÄŸu sabitle (Padding)
    padded = pad_sequences(sequences, maxlen=MAX_LEN)
    
    # 2. Tahmin (Prediction)
    prediction = model.predict(padded, verbose=0)[0][0]
    
    # 3. SonuÃ§ Yorumlama
    print(f"\nğŸ” Analiz Edilen: {clean_url}")
    print(f"ğŸ“Š Phishing Skoru: %{prediction * 100:.2f}")
    
    if prediction > 0.5:
        print("SONUÃ‡: TEHLÄ°KELÄ° (PHISHING) ")
        print("   -> Bu site bilgilerinizi Ã§almaya Ã§alÄ±ÅŸabilir!")
    else:
        print("SONUÃ‡: GÃœVENLÄ° (BENIGN)")
        print("   -> Temiz gÃ¶rÃ¼nÃ¼yor.")
    print("-" * 40)


def main():
    model, tokenizer = load_ai_assets()
    
    print("Ã‡Ä±kmak iÃ§in 'q' veya 'exit' yazÄ±n.")
    print("-" * 40)

    while True:
        url = input("ğŸ”— Kontrol edilecek URL'i girin: ")
        
        if url.lower() in ['q', 'exit', 'quit']:
            print("ğŸ‘‹ GÃ¼le gÃ¼le!")
            break
        
        if len(url.strip()) == 0:
            continue
            
        predict_url(url, model, tokenizer)

if __name__ == "__main__":
    main()